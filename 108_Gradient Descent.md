# 학습내용

- 이전 시간의 비용함수가 전체 데이터셋의 예측이 얼마나 잘 평가되었는지 보는 것이라면, 경사하강법은 이를 가능케하는 파라미터 w와 b를 찾아내는 방법 중 하나 입니다.
- 우선, 비용 함수는 볼록한 형태여야 합니다. 볼록하지 않은 함수를 쓰게 되면, 경사하강법을 통해 최적의 파라미터를 찾을 수 없습니다.
- 함수의 최소값을 모르기 때문에, 임의의 점을 골라서 시작합니다.
- 경사하강법은 가장 가파른(steepest) 방향, 즉 함수의 기울기를 따라서 최적의 값으로 한 스텝씩 업데이트하게 됩니다.
- 알고리즘은 아래와 같습니다.
<br>&nbsp; w : w − α (dJ(w,b) / dw)
<br>&nbsp; b : b − α (dJ(w,b) / db)  
<br>&nbsp; α : 학습률이라고 하며, 얼만큼의 스텝으로 나아갈 것인지 정합니다.  
<br>&nbsp; dJ(w) / dw : 도함수(미분계수)라고 하며, 미분을 통해 구한 값(기울기) 입니다. dw 라고 표기하기도 합니다.

- 만약 dw >0 이면, 파라미터 w 는 기존의 w 값 보다 작은 방향으로 업데이트 될 것이고, 만약 dw <0 이면, 파라미터 w 는 기본의 w 값 보다 큰 방향으로 업데이트 될 것입니다.
- 도함수는 함수의 기울기라고 볼 수 있습니다. 다음 시간에 조금 더 자세히 설명하겠습니다.

- 하나의 변수에 대한 도함수는  dw = df(w) / dw 라고 표기하지만 두 개 이상은 보통 아래와 같이 표현 합니다.
<br>&nbsp; dw = ∂J(w,b) / ∂w : 함수의 기울기가 w 방향으로 얼만큼 변했는지 나타냅니다.
<br>&nbsp; db = ∂J(w,b) / ∂b : 함수의 기울기가 b 방향으로 얼만큼 변했는지 나타냅니다.


![image](https://user-images.githubusercontent.com/52098725/92113622-8bd12c80-ee2a-11ea-9c61-1b302cb0f710.png)
- 비용 함수 J(w,b)가 볼록하다는 사실이 로지스틱 회귀에 위의 비용 함수 J를 사용한 큰 이유 중 하나임.

![image](https://user-images.githubusercontent.com/52098725/92114429-c8e9ee80-ee2b-11ea-8866-c27a6eb285dd.png)
- w := w − α (dJ(w,b) / dw)
- " := "는 값을 갱신한다는 의미
- "∂"는 d와 비슷한 뜻으로서, 편미분을 의미한다.
